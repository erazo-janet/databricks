![image](https://github.com/user-attachments/assets/04103854-b05c-438f-8124-b7d3b740aacc)

all data stays with youir clouyd platform 
databricks application operates in control pane 

![image](https://github.com/user-attachments/assets/e760d583-c57d-4ecd-82ae-b33091935ee5)

![image](https://github.com/user-attachments/assets/574ede8c-b390-4985-9ca1-d1df5ff40e6a)
serverless compute is avalaible for everwyone - compute clusters are set aside for you to setup your work, all managed by databricks
![image](https://github.com/user-attachments/assets/89cc08ff-dce2-46a9-ba61-acb4dc2f5a96)


![image](https://github.com/user-attachments/assets/0b304bc2-2607-470f-aabe-07631f53c8d4)


Databricks: A unified analytics platform optimized for Apache Spark, enabling data science, data engineering, and machine learning workflows

Workspace: A collaborative environment within Databricks where users can create, manage, and share notebooks, clusters, and data

Notebook: A web-based interface in Databricks used for writing and executing code, visualizing results, and collaborating on data science workflows

Cluster: A set of virtual machines (VMs) running in the cloud to perform data processing tasks in parallel. Clusters are used to run computations for data processing and machine learning

Job: A set of tasks executed in a cluster, which includes one or more notebooks or Python/Scala/SQL scripts

Delta Lake: An open-source storage layer that provides ACID transactions, data schema enforcement, and quality improvements for big data lakes

Spark: An open-source distributed computing system used for data processing and analytics on big data, with support for batch and real-time processing

MLflow: An open-source platform for managing the machine learning lifecycle, including experimentation, reproducibility, and deployment

Databricks Runtime: The underlying compute environment that runs workloads on Databricks, optimized for Spark and other big data tasks

Delta Tables: A data storage format within Delta Lake that supports efficient data management through ACID transactions, schema evolution, and streaming updates

Notebook Libraries: Pre-built libraries or modules that can be imported into Databricks notebooks for use in machine learning, data science, or other data-related tasks

Databricks SQL: A high-performance querying engine for data analytics within Databricks, allowing SQL-based data exploration and visualization

Autoscaling: The ability of Databricks clusters to automatically adjust the number of nodes based on workload demand to optimize performance and cost

Data Pipelines: Workflows in Databricks that manage the extraction, transformation, and loading (ETL) of data into a usable format for analysis



data governance:
![image](https://github.com/user-attachments/assets/1cc9ce49-3f61-4c0a-b1bf-84eaba9a144c)

databricks operate on top of your cloud data storage. your data lives there, and databricks supplies delta lake to have universal formatting option
unity catalog - open source security and governance layer to manage data, user security and more
delta sharing - seccure method for sharing data (with vendors, external clients, etc)

nity Catalog: A unified data governance solution within Databricks that provides fine-grained access control, data lineage, and metadata management across various data sources, enabling secure and collaborative data usage.

Delta Live Tables (DLT): A feature in Databricks that simplifies the development, deployment, and maintenance of data pipelines. It allows users to build reliable and scalable streaming or batch pipelines using Delta tables, automatically handling data quality and schema enforcement.

Databricks Lakehouse: A data architecture that combines the best of data lakes and data warehouses, using Databricks for data engineering, analytics, and machine learning on a unified platform optimized for performance, security, and scalability.
